{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 07. Neural networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Linear Classification Problem\n",
    "When we want to use machine learning to build a car image classifier, we need a training dataset with true labels, a car or not a car. After learning process, we get a good classifier. When we test it with a new image, the classifier will answer whether this new image is a car or not.\n",
    "![](../../img/lec07_01.png)\n",
    "\n",
    "Before moving on, we need to know how a computer ‘sees’ a picture. Like the picture in the right, a computer always ‘sees’ an image as a bunch of pixels with intensity values. For example, the picture shows the position of a pixel (red point) and its intensity value is 69.\n",
    "\n",
    "![](../../img/lec07_02.png)\n",
    "\n",
    "Now, let’s see the learning process. First, take two pixels as features.\n",
    "\n",
    "![](../../img/lec07_03.png)\n",
    "\n",
    "Second, define a **non-linear logistic regression** as a hypothesis ***H***. Our goal is to find a good ***H*** which can distinguish positive data and negative data well.\n",
    "\n",
    "![](../../img/lec07_04.png)\n",
    "\n",
    "Last, learn all parameters of ***H***. Compare this with linear logistic regression, the non-linear form is more complex since it is with lots of polynomial terms.\n",
    "\n",
    "However, when a number of feature is large, the above solution is not a good choice to learn complex non-linear hypothesis. Suppose we have a 50x50 pixels image and all pixels are features, hence, a non-linear hypothesis must have more than 2500 features since ***H*** has extra quadratic or the cubic features. The computation cost would be very expensive in order to find all parameters $\\theta$ (or $W$ as previously) of these features per the training data.\n",
    "\n",
    "Therefore, we need a better way — Neural Network, which is a very powerful and widely used model to learn a complex non-linear hypothesis for many applications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network (NN)\n",
    "\n",
    "In this section, we are going to talking about how to represent hypothesis when using neural networks.\n",
    "\n",
    "Originally, Neural Network is an algorithm inspired by human brain that tries to mimic a human brain. Like human brain’s neurons, NN has a lots of interconnected nodes (a.k.a neurons) which are organized in layers.\n",
    "\n",
    "### Simplest Neural Network\n",
    "\n",
    "This simplest NN model only contains a neuron. **We can treat a neuron (node) as a logistic unit with *Sigmoid (logistic) Activation Function****,* which can output a computation value based on sigmoid activation function.\n",
    "\n",
    "* The terminology of parameters $\\theta$ in ** NN ** is called ***weights***\n",
    "* Depending on the problems, you can decide whether to use the bias units or not.\n",
    "\n",
    "![](../../img/lec07_05.png)\n",
    "\n",
    "### Neural Network (NN)\n",
    "\n",
    "* Layer 1 is called **Input Layer** that inputs features.\n",
    "* Last Layer is called **Output Layer** that outputs the final value computed by hypothesis ***H***.\n",
    "* The layer between Input Layer and Output Layer is called **Hidden Layer**, which is a block we group neurons together.\n",
    "\n",
    "![](../../img/lec07_06.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "\n",
    "A neuron (node) is actually a logistic unit with Sigmoid (logistic) Activation Function(i.e simple logistic regression). If you are familiar with Linear Algebra, you can think of a hidden layer as a linear combination of previous layer’s nodes. **Therefore, the core idea of NN is to solve complex non-linear classification problem by using many sequences of simple logistic regression.**\n",
    "\n",
    "In order to get a clear picture about what this neural network is doing, let’s go through the computational steps and visualize them.\n",
    "\n",
    "**First**, we visualize the transition process of matrix $\\theta$, which is a controlling function mapping from layer *j* to *j+1.*\n",
    "\n",
    "![](../../img/lec07_07.png)\n",
    "![](../../img/lec07_08.png)\n",
    "\n",
    "**Second**, we visualize each computation process of neurons. Note that the output value of each neurons is calculated by its sigmoid activation function.\n",
    "\n",
    "![](../../img/lec07_09.png)\n",
    "![](../../img/lec07_10.png)\n",
    "![](../../img/lec07_11.png)\n",
    "![](../../img/lec07_12.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Other Neural Network Architectures\n",
    "\n",
    "Other Neural Network Architectures can be designed by extending hidden layers. The number of neurons per layer will be based on the problems.\n",
    "![](../../img/lec07_13.png)\n",
    "\n",
    "### Applications and Examples\n",
    "\n",
    "Based on above concept, we are going to design some NN to show that how an NN model can be applied to non-linear classification problems.\n",
    "\n",
    "#### Examples1 — AND\n",
    "\n",
    "We can design a simple NN with single neuron for solving AND problem.  Given -30, 20 and 20 as weights, the *Sigmoid Activation Function* ***H*** of this neuron (node) can be specified. When we predict data using this ***H,*** we can get a perfect result.\n",
    "\n",
    "![](../../img/lec07_14.png)\n",
    "![](../../img/lec07_15.png)\n",
    "\n",
    "\n",
    "#### Examples2 — OR\n",
    "\n",
    "The concept of OR operation is similar to AND, but we change the weight of the bias unit as -10.\n",
    "\n",
    "![](../../img/lec07_16.png)\n",
    "![](../../img/lec07_17.png)\n",
    "\n",
    "#### Examples3 — Negation\n",
    "![](../../img/lec07_18.png)\n",
    "\n",
    "#### Examples4 — NAND\n",
    "![](../../img/lec07_19.png)\n",
    "\n",
    "#### Examples5 — XOR\n",
    "\n",
    "Clearly, this is a non-linear classification problem, we can solve it by using a non-linear logistic regression ***H*** that we discussed in the beginning.\n",
    "![](../../img/lec07_20.png)\n",
    "![](../../img/lec07_21.png)\n",
    "\n",
    "However, when the number of features and data is large, The ***H*** will be too complex to understand and the computation cost is expensive. \n",
    "\n",
    "**Instead, we use NN structure to make model *H* more clear and simple.** Here, we ****** apply NN to XOR Problem based on AND, NAND and OR.\n",
    "![](../../img/lec07_22_.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Multi-Class Classification Problem\n",
    "\n",
    "In this case, we can add nodes in the Output Layer, each node can predict one class, the concept of this is similar to one-vs-all mechanism.\n",
    "\n",
    "![](../../img/lec07_23.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "\n",
    "The steps are described as below:\n",
    "\n",
    "* Firstly, we need a weighting matrix $\\theta$ (or denoted W) for each hidden layer. We can derive them randomly or per prior-knowledge.\n",
    "* Start from Input Layer X (or denoted $a^1$).\n",
    "* Do Forward Propagation throughout the Output Layer and get the final value.\n",
    "* Use the final value for prediction.\n",
    "\n",
    "![](../../img/lec07_24.png)\n",
    "\n",
    "\n",
    "### More Details\n",
    "\n",
    "Let’s take binary classification problem as example and visualize the detail of Forward Propagation. First of all, we need a completed NN architecture, that is, all weighting matrices **W** are known already.\n",
    "\n",
    "Input Layer: 4 units, Output Layer: 1 units, Hidden Layer : 6 units.\n",
    "\n",
    "![](../../img/lec07_25.png)\n",
    "![](../../img/lec07_26.png)\n",
    "\n",
    "**Forward Propagation (1)— From Input Layer to Output Layer**\n",
    "\n",
    "![](../../img/lec07_27.png)\n",
    "![](../../img/lec07_28.png)\n",
    "\n",
    "**Forward Propagation(2) — From Hidden Layer to Output Layer**\n",
    "\n",
    "![](../../img/lec07_29.png)\n",
    "\n",
    "In the end, we can get the final value from Output Layer, and use it for prediction.\n",
    "\n",
    "* If final value ≥ 0.5, we predict the label y=1.\n",
    "* If not, then we predict the label y=0.\n",
    "\n",
    "![](../../img/lec07_30.png)\n",
    "\n",
    "### **Multi-Classification Problem**\n",
    "\n",
    "In multi-classification problem, there are more than one unit in the Output Layer. Each unit represents **how certain the data belongs to a category.** In this situation, we can apply the one-vs-all strategy to decide the prediction result. Note that the Forward Propagation is same as binary classification problem.\n",
    "\n",
    "![](../../img/lec07_31.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Loss Function\n",
    "\n",
    "Loss Function is also called Cost Function, but this name is used more commonly in NN.\n",
    "\n",
    "The goal of Loss Function is to measure the error that the model made. Here, we give a generated formula about loss function. Note that a node of NN is a logistic unit with Sigmoid (logistic) Activation Function.\n",
    "\n",
    "![](../../img/lec07_32.png)\n",
    "\n",
    "## Backward Propagation\n",
    "\n",
    "This procedure is for **minimizing Loss Function**, just like the technique — gradient descent we used in previous note.\n",
    "\n",
    "The way we used here is similar to Gradient Descent and there are two steps:\n",
    "\n",
    "1. Compute the partial derivative of $J(\\theta)$ or $J(W)$\n",
    "2. Update each element of weighting matrix **$\\theta$**.\n",
    "\n",
    "![](../../img/lec07_33.png)\n",
    "\n",
    "## Understanding with an Intuitive Way\n",
    "\n",
    "Let’s visualize the procedure by using the result of partial derivative of $J(\\theta)$\n",
    "\n",
    "Let’s start from another example, assuming weighting matrices **$\\theta$** (or denoted **$W$**) has been initialized. Our goal is to minimize the **$J(\\theta)$** and then update the weighting matrices **$\\theta$**.\n",
    "\n",
    "Note — We can use random method for initializing the weighting matrices Θ if we have no prior knowledge of the problem.\n",
    "\n",
    "![](../../img/lec07_34.png)\n",
    "\n",
    "After Calculus calculation, we can get the result of partial derivative of **$J(\\theta)$**\n",
    "\n",
    "![](../../img/lec07_35.png)\n",
    "![](../../img/lec07_36.png)\n",
    "\n",
    "\n",
    "Let’s dig into the meaning of **$\\delta$**-   \n",
    "\n",
    "* **$\\delta^3$** is the error of layer 3, the Output Layer in this example.\n",
    "* **$\\delta^2$** is the error of layer 2, the Hidden Layer in this example.\n",
    "* **$\\delta^1$** is not exist since layer 1 is the Input Layer.\n",
    "\n",
    "![](../../img/lec07_37.png)\n",
    "![](../../img/lec07_38.png)\n",
    "\n",
    "With the result of partial derivative of $J(\\theta)$, now you can update the weighting matrix Θ.\n",
    "\n",
    "![](../../img/lec07_39.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## More Details about Partial Derivative of $J(\\theta)$\n",
    "\n",
    "In this section, we want to give some hints about the derivations of the formulation above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![](../../img/lec07_40.png)\n",
    "\n",
    "For convenience’s sake, we use the same example above and suppose there is only one data. Hence, the loss function $J(\\theta)$ is simpler than the general form.\n",
    "![](../../img/lec07_41.png)\n",
    "\n",
    "In order to do the backward propagation, we need to do the forward\n",
    "propagation first.\n",
    "\n",
    "![](../../img/lec07_42.png)\n",
    "\n",
    "Then we can do Partial Derivative of J(Θ). Here, we show the Partial\n",
    "Derivative of two elements of W¹ and W² respectively. If you are\n",
    "interested, you can solve it with hints.\n",
    "\n",
    "![](../../img/lec07_43.png)\n",
    "\n",
    "![](../../img/lec07_44.png)\n",
    "\n",
    "Finally, you can prove the formula of partial derivative of J(Θ) if you follow the procedure above. Don’t worry about it, if you are not familiar with Calculus, we have given you enough concepts in the section of ***Understanding with an Intuitive Way.***\n",
    "\n",
    "## How to implement NN in practice?\n",
    "\n",
    "<a href=\"https://medium.com/@qempsil0914/implement-neural-network-without-using-deep-learning-libraries-step-by-step-tutorial-python3-e2aa4e5766d1\" class=\"bb cn kh ki kj kk\">Here is the reference for exercise</a>,\n",
    "we implement NN without using deep learning library. We hope the\n",
    "tutorial can help you understand the architecture of NN and the Forward\n",
    "and Backward Propagation procedure more clearly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:0.4692677031181711\n",
      "Error:0.007389464275819224\n",
      "Error:0.0049285194301067325\n",
      "Error:0.00392037855516743\n",
      "Error:0.0033414788210913435\n",
      "Error:0.002955589718519708\n",
      "Error:0.0026754446457782103\n",
      "Error:0.0024604603544171283\n",
      "Error:0.002288910989589701\n",
      "Error:0.0021479944518608463\n",
      "[[1.67085900e-05]\n",
      " [9.86968263e-01]\n",
      " [7.75451681e-01]\n",
      " [2.71376581e-01]\n",
      " [1.55096932e-03]\n",
      " [2.06919456e-05]]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def activation(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigma_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "\n",
    "# X - это матрица, где столбцы это признаки,\n",
    "# а строки это объекты выборки.\n",
    "X = np.array([[0, 0, 1],\n",
    "              [0.3, 1, 0],\n",
    "              [1, 0.3, 1],\n",
    "              [0.6, 0.2, 0],\n",
    "              [0.6, 0.2, 1]])\n",
    "\n",
    "# X = np.array([[0, 0, 1],\n",
    "#               [0.3, 1, 0],\n",
    "#               [1, 0.3, 0],\n",
    "#               [0.6, 0.2, 1],\n",
    "#               [0.6, 0.2, 1]])\n",
    "\n",
    "# X - это матрица, где столбцы это признаки\n",
    "# (в данном случае один целевой признак),\n",
    "# а строки это объекты выборки.\n",
    "y = np.array([[0],\n",
    "              [1],\n",
    "              [1],\n",
    "              [0],\n",
    "              [0]])\n",
    "\n",
    "np.random.seed(4)\n",
    "\n",
    "# Нотация слоев - L1, L2, L3.\n",
    "# В каждом слое есть n1, n2, n3 нейронов.\n",
    "# Строки в матрицах индексируются как i, столбцы как j.\n",
    "\n",
    "# W_1_2 - это матрица весов между первым и вторым слоем.\n",
    "# Строки определяют левый нейрон (т.е. нейрон первого слоя) и соответственно количество строк равно n1.\n",
    "# Столбцы определяют правый нейрон (т.е. нейрон второго слоя) и соответственно количество столбцов равно n2.\n",
    "# На пересечении i-ой строки и j-го столбца получаем ячейку с конкретным весом, который\n",
    "# связывает i-ый левый нейрон (слоя L1) и j-ый правый нейрон (слоя L2)\n",
    "W_1_2 = 2 * np.random.random((3, 5)) - 1\n",
    "\n",
    "# W_2_3 - это матрица весов между вторым и третьим слоем. Все остальное как в W_1_2\n",
    "W_2_3 = 2 * np.random.random((5, 1)) - 1\n",
    "\n",
    "speed = 1.1\n",
    "\n",
    "for j in range(100000):\n",
    "    # lo, l1, l2 - матрицы определенного слоя сети. Каждая строка матрицы это реакция на\n",
    "    # i-ый объект входа. Каждая колонка матрицы это реакиця j-го нейрона соответствующего слоя на разные входные образы.\n",
    "    # На пересечении i-ой строки и j-го столбца получаем ячейку с конкретной реакцией j-го нейрона на конкретный вход.\n",
    "\n",
    "\n",
    "    # Первый слой нейронов полностю принимает значения входа Х (т.е. все реакции единичные).\n",
    "    l1 = X\n",
    "\n",
    "    # Второй слой нейронов расчитывается как функция активации по каждому элементу матрицы U\n",
    "    # По сути l2 это матрица 4 на 4, где в каждой ячейке результат актиации нейрона второго слоя для всех 4-ех входных образов\n",
    "    # Детально:\n",
    "    # матрица U = np.dot(l0, W_0_1) является результатом\n",
    "    # матричного произведения выходов нейронов предыдущего слоя на веса между 1 и 2 слоем.\n",
    "    # Строки матрицы U отвечают за конкретный входной образ (объект).\n",
    "    # Столбцы матрицы U отвечают за нейроны правого слоя (L2).\n",
    "    # На пересечении i-ой строки и j-го столбца получаем ячейку с конкретной взвешенной суммой\n",
    "    # для i-го входного образа и j-го нейрона слоя (l2). Иными словами для каждого нейрона слоя L2 и для каждого входа\n",
    "    # считается U = W1X1 + W2X2 + W3X3 (поскольку входной нейрон содержит 3 нейрона).\n",
    "    l2 = activation(np.dot(l1, W_1_2))\n",
    "\n",
    "    # тоже самое проделываем для Третьего слоя\n",
    "    # По сути l3 это матрица 4 на 1, где в каждой ячейке результат актиации нейрона третьего слоя (а он один)\n",
    "    #  для всех 4-ех входных образов\n",
    "    l3 = activation(np.dot(l2, W_2_3))\n",
    "\n",
    "    # расчитывает ошибку на выходе\n",
    "    l3_error = y - l3\n",
    "\n",
    "    # расчитывает модуль средней ошибки\n",
    "    if (j % 10000) == 0:\n",
    "        print(\"Error:\" + str(np.mean(np.abs(l3_error))))\n",
    "\n",
    "    # sigma - есть локальный градиент ошибки\n",
    "\n",
    "    # l3_sigma расчитывается как ошибка выхода всей сети на производную функции активации всех нейронов L3.\n",
    "    # На пересечении i-ой строки и j-го столбца получаем ячейку с конкретной сигмой\n",
    "    # для i-го входного образа и j-го нейрона слоя (l3). Иными словами для каждого нейрона слоя L3\n",
    "    # и для каждого входа расчитывается производная по функции активации умноженная на ошибку.\n",
    "    # То есть l3_sigma будет матрица 4 на 1 (поскольку в L3 только один нейрон).\n",
    "    l3_sigma = l3_error * sigma_derivative(l3)\n",
    "\n",
    "    # Ошибка L2 слоя оценивается через взвешенную сигму слоя L3 по весам между L2 и L3\n",
    "    # Поскольку l3_sigma это матрица 4 на 1 и матрица W_2_3 4 на 1, то последнюю матрицу\n",
    "    # надо Транспонировать, чтобы выполнить правило умножения матриц и взвесить элементы l3_sigma\n",
    "    # по элементам W_2_3.\n",
    "    # Тогда итоговая матрица l2_error будет 4 на 4, где на пересечении i-ой строки и j-го столбца\n",
    "    # будет ячейка с конкретной ошибкой j-го нейрона слоя L2 для i-ого входного образа.\n",
    "    l2_error = l3_sigma.dot(W_2_3.T)\n",
    "\n",
    "    # l2_sigma расчитывается как ошибка слоя L2 на производную функции активации всех нейронов L2.\n",
    "    # На пересечении i-ой строки и j-го столбца получаем ячейку с конкретной сигмой\n",
    "    # для i-го входного образа и j-го нейрона слоя (L2). Иными словами для каждого нейрона слоя L2\n",
    "    # и для каждого входа расчитывается производная по функции активации умноженная на ошибку.\n",
    "    # То есть l2_sigma будет матрица 4 на 4 (поскольку в L2 четыре нейрона).\n",
    "    l2_sigma = l2_error * sigma_derivative(l2)\n",
    "\n",
    "    # обновляем веса\n",
    "\n",
    "    # l2 это матрица 4 на 4, где в каждой ячейке результат актиации нейрона второго слоя для\n",
    "    # всех 4-ех входных образов (по строкам).\n",
    "    # А l3_sigma это матрица 4 на 1, где в каждой строке локальный градиент ошибки для 4-ех входных образов.\n",
    "    # Чтобы взвесить столбец матрицы l2 по локальному градиенту (l3_sigma) нужно транспонировать матрицу l2, чтобы\n",
    "    # результат активации каждого нейрона в слое L2 по каждому входному образу вытянулся в строку.\n",
    "    # Тогда соответствующая строка умножится на столбик l3_sigma.\n",
    "    # Заметьте, что транспонирование l3_sigma не даст результата, так как тогда в каждом столбце\n",
    "    # l3_sigma будет по одному значению, а в каждой строке l2 по 4 значения, а значит перемножение матриц не пройдет.\n",
    "    W_2_3 += speed * l2.T.dot(l3_sigma)\n",
    "\n",
    "    # аналогично W_2_3\n",
    "    W_1_2 += speed * l1.T.dot(l2_sigma)\n",
    "\n",
    "\n",
    "\n",
    "# Прямое распространение для тестовых данных\n",
    "X_test = np.array([[0, 0, 0],\n",
    "                   [0.6, 0.8, 1],\n",
    "                   [0.6, 0.6, 1],\n",
    "                   [1, 1, 0],\n",
    "                   [0.1, 0.1, 0],\n",
    "                   [0.2, 0.2, 1]])\n",
    "\n",
    "# Y_test должен получиться [0, 1, 1, 1, 0, 0]\n",
    "\n",
    "l1 = X_test\n",
    "l2 = activation(np.dot(l1, W_1_2))\n",
    "l3 = activation(np.dot(l2, W_2_3))\n",
    "print(l3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links: https://medium.com/@qempsil0914/courseras-machine-learning-notes-week5-neural-network-lost-function-forward-and-backward-8b293401e4dc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
